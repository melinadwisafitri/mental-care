{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN6D9wed3huIQ8HN+a6A7hK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/melinadwisafitri/mental-care/blob/machine-learning/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwliOeX1xdmU",
        "outputId": "db7af27b-3a76-4c98-d846-44dd8a372eeb"
      },
      "source": [
        "import urllib.request\n",
        "import json\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords,wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTZ1m1lryrbc",
        "outputId": "f7768143-8246-4d8a-cc9b-3ca2e329bc1d"
      },
      "source": [
        "data_url = 'https://raw.githubusercontent.com/melinadwisafitri/mental-care/machine-learning/dataset/data.json'\n",
        "urllib.request.urlretrieve(data_url, 'data.json')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('data.json', <http.client.HTTPMessage at 0x7f03ff3948d0>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZi4vezGzGB6"
      },
      "source": [
        "with open('data.json', 'r') as f:\n",
        "  data = json.load(f)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDsKuUtAzrYc",
        "outputId": "2b4692f0-6034-438f-8776-a4d07dfb9194"
      },
      "source": [
        "words=[]\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_letters = ['!', '?', ',', '.']\n",
        "for data in data['mental_healths']:\n",
        "    for question in data['question']:\n",
        "        word = nltk.word_tokenize(question)\n",
        "        words.extend(word)        \n",
        "        documents.append((word, data['tag']))\n",
        "        if data['tag'] not in classes:\n",
        "          classes.append(data['tag'])\n",
        "print(documents)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['hai'], 'salam_pembuka'), (['hallo'], 'salam_pembuka'), (['apa', 'kabar', '?'], 'salam_pembuka'), (['terimakasih'], 'salam_penutup'), (['sampai', 'ketemu', 'lagi'], 'salam_penutup'), (['Apa', 'saja', 'jenis-jenis', 'gangguan', 'mental', '?'], 'pengertian_mental_health'), (['Apa', 'saja', 'gejala-gejala', 'gangguan', 'mental', '?'], 'gelaja_kesehatan_mental'), (['Apakah', 'kesehatan', 'mental', 'itu', 'penting', '?'], 'pentingnya_kesehatan_mental'), (['Bagaimana', 'cara', 'menjaga', 'kesehatan', 'mental', '?'], 'menjaga_kesehatan_mental'), (['Bagaimana', 'menjaga', 'kesehatan', 'mental', 'diusia', 'remaja', '?'], 'menjaga_kesehatan_mental_remaja'), (['Apa', 'saja', 'ciri-ciri', 'mental', 'yang', 'sehat', '?'], 'ciri_kesehatan_mental'), (['Bagaimana', 'cara', 'meningkatkan', 'self', 'awareness', '?'], 'self_awareness'), (['Bagaimana', 'melewati', 'hari', 'tanpa', 'menyakiti', 'diri', '?'], 'self_awareness'), (['Bagaimana', 'cara', 'mengendalikan', 'emosi', '?'], 'emosi'), (['Bagaimana', 'cara', 'meningkatkan', 'self', 'efficacy/keyakinan', 'diri', 'dan', 'melakukan', 'perencanaan', 'dengan', 'teknik', 'SMART', '?'], 'keyakinan_diri'), (['Jika', 'masih', 'tetap', 'belum', 'memiliki', 'keyakinan', 'untuk', 'tetap', 'melakukan', ',', 'apa', 'rencana', 'tersebut', 'harus', 'diurungkan', 'atau', 'tetap', 'optimis', 'dilakukan', '?'], 'keyakinan_diri'), (['Bagaimana', 'cara', 'bersabar', '?'], 'sabar'), (['Bagaimana', 'meningkatkan', 'kepercayaan', 'diri', '?'], 'percaya_diri'), (['Bagaimana', 'cara', 'berkomunikasi', 'yang', 'baik', '?'], 'komunikasi'), (['Apa', 'itu', 'gangguan', 'bipolar', '?'], 'bipolar'), (['Apa', 'yang', 'dimasukan', 'dengan', 'Skizofrenia', '?'], 'skizofrenia'), (['Apakah', 'bebagi', 'cerita/curhat', 'dengan', 'orang', 'lain', 'itu', 'penting', '?'], 'curhat'), (['Apa', 'saja', 'manfaat', 'dari', 'curhat', 'atau', 'bercerita', '?'], 'curhat'), (['Apakah', 'ada', 'undang-undang', 'mengenai', 'kesehatan', 'jiwa', '?'], 'hukum_kesehatan_jiwa'), (['Apa', 'saja', 'faktor-faktor', 'yang', 'mempengaruhi', 'kesehatan', 'mental', '?'], 'faktor_mental_health'), (['Hal', 'apa', 'yang', 'dapat', 'kita', 'lakukan', 'sebagai', 'pertolong', 'pertama', 'untuk', 'kerabat', 'atau', 'teman', 'yang', 'terkena', 'gangguan', 'mental', '?'], 'pertolongan_pertama'), (['Bagaimana', 'cara', 'meningkatkan', 'kesadaran', 'masyarakat', 'terhadap', 'gangguan', 'Mental', '?'], 'kesadaran_kesehatan_mental'), (['Kenapa', 'saya', 'tiba-tiba', 'merasa', 'sedih', '?'], 'sedih'), (['Bagaimana', 'cara', 'mengatasi', 'supaya', 'tidak', 'nervous', '?'], 'mengatasi_nervous'), (['pengaruh', 'faktor', 'lingkungan'], 'faktor_lingkungan'), (['Apakah', 'sosial', 'media', 'berpengaruh', 'dengan', 'kesehatan', 'mental', '?'], 'sosial_media'), (['Apa', 'itu', 'mental', 'breakdown', '?'], 'mental_breakdown'), (['Apa', 'saja', 'yang', 'menyebabkan', 'gangguan', 'kesehatan', 'mental', '?'], 'penyebab_gangguan_mental'), (['Apa', 'saja', 'faktor', 'risiko', 'gangguan', 'kesehatan', 'mental', '?'], 'faktor_resiko_gangguan_mental'), (['Apa', 'saja', 'peencegahan', 'yang', 'dapat', 'dilakukan', 'untuk', 'mengurangi', 'masalah', 'terhadap', 'kesahatan', 'mental', '?'], 'pencegahan'), (['Bagaimana', 'mengobati', 'gangguan', 'kesehatan', 'mental', '?'], 'solusi_kesehatan_mental')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5W3Mn_OC6Oa",
        "outputId": "c7c73838-e2c9-40d5-aaed-b9230d90aeb3"
      },
      "source": [
        "import pickle\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_letters]\n",
        "words = sorted(list(set(words)))\n",
        "classes = sorted(list(set(classes)))\n",
        "print (len(documents), \"documents\")\n",
        "print (len(classes), \"classes\", classes)\n",
        "print (len(words), \"unique lemmatized words\", words)\n",
        "pickle.dump(words,open('words.pkl','wb'))\n",
        "pickle.dump(classes,open('classes.pkl','wb'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36 documents\n",
            "30 classes ['bipolar', 'ciri_kesehatan_mental', 'curhat', 'emosi', 'faktor_lingkungan', 'faktor_mental_health', 'faktor_resiko_gangguan_mental', 'gelaja_kesehatan_mental', 'hukum_kesehatan_jiwa', 'kesadaran_kesehatan_mental', 'keyakinan_diri', 'komunikasi', 'mengatasi_nervous', 'menjaga_kesehatan_mental', 'menjaga_kesehatan_mental_remaja', 'mental_breakdown', 'pencegahan', 'pengertian_mental_health', 'pentingnya_kesehatan_mental', 'penyebab_gangguan_mental', 'percaya_diri', 'pertolongan_pertama', 'sabar', 'salam_pembuka', 'salam_penutup', 'sedih', 'self_awareness', 'skizofrenia', 'solusi_kesehatan_mental', 'sosial_media']\n",
            "113 unique lemmatized words ['ada', 'apa', 'apakah', 'atau', 'awareness', 'bagaimana', 'baik', 'bebagi', 'belum', 'bercerita', 'berkomunikasi', 'berpengaruh', 'bersabar', 'bipolar', 'breakdown', 'cara', 'cerita/curhat', 'ciri-ciri', 'curhat', 'dan', 'dapat', 'dari', 'dengan', 'dilakukan', 'dimasukan', 'diri', 'diurungkan', 'diusia', 'efficacy/keyakinan', 'emosi', 'faktor', 'faktor-faktor', 'gangguan', 'gejala-gejala', 'hai', 'hal', 'hallo', 'hari', 'harus', 'itu', 'jenis-jenis', 'jika', 'jiwa', 'kabar', 'kenapa', 'kepercayaan', 'kerabat', 'kesadaran', 'kesahatan', 'kesehatan', 'ketemu', 'keyakinan', 'kita', 'lagi', 'lain', 'lakukan', 'lingkungan', 'manfaat', 'masalah', 'masih', 'masyarakat', 'medium', 'melakukan', 'melewati', 'memiliki', 'mempengaruhi', 'mengatasi', 'mengenai', 'mengendalikan', 'mengobati', 'mengurangi', 'meningkatkan', 'menjaga', 'mental', 'menyakiti', 'menyebabkan', 'merasa', 'nervous', 'optimis', 'orang', 'peencegahan', 'pengaruh', 'penting', 'perencanaan', 'pertama', 'pertolong', 'remaja', 'rencana', 'risiko', 'saja', 'sampai', 'saya', 'sebagai', 'sedih', 'sehat', 'self', 'skizofrenia', 'smart', 'sosial', 'supaya', 'tanpa', 'teknik', 'teman', 'terhadap', 'terimakasih', 'terkena', 'tersebut', 'tetap', 'tiba-tiba', 'tidak', 'undang-undang', 'untuk', 'yang']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNG3J6kTDLQF",
        "outputId": "f2126828-8b04-4b1f-df98-6cca2d059232"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "training = []\n",
        "output_empty = [0] * len(classes)\n",
        "for doc in documents:\n",
        "    bag = []\n",
        "    pattern_words = doc[0]\n",
        "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "x_train = list(training[:,0])\n",
        "y_train = list(training[:,1])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "882t0m0YDgQJ",
        "outputId": "b1a8afc4-0a10-49c8-b888-754951c2cd50"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "import tensorflow as tf\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(64, activation='relu'))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),\n",
        "    tf.keras.layers.LSTM(64),\n",
        "    tf.keras.layers.Dense(128,input_shape=(len(x_train[0]),), activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(len(y_train[0]), activation='softmax')\n",
        "])\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "hist = model.fit(np.array(x_train), np.array(y_train), epochs=200, batch_size=5, verbose=1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "8/8 [==============================] - 3s 51ms/step - loss: 3.4045 - accuracy: 0.0278\n",
            "Epoch 2/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 3.3979 - accuracy: 0.0556\n",
            "Epoch 3/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 3.4020 - accuracy: 0.0556\n",
            "Epoch 4/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 3.4020 - accuracy: 0.0556\n",
            "Epoch 5/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 3.3872 - accuracy: 0.0833\n",
            "Epoch 6/200\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 3.3948 - accuracy: 0.0833\n",
            "Epoch 7/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 3.4058 - accuracy: 0.1111\n",
            "Epoch 8/200\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 3.3874 - accuracy: 0.0556\n",
            "Epoch 9/200\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 3.4016 - accuracy: 0.0556\n",
            "Epoch 10/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 3.3642 - accuracy: 0.0833\n",
            "Epoch 11/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 3.3696 - accuracy: 0.0278\n",
            "Epoch 12/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 3.4279 - accuracy: 0.0278\n",
            "Epoch 13/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 3.3778 - accuracy: 0.0556\n",
            "Epoch 14/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 3.3906 - accuracy: 0.0833\n",
            "Epoch 15/200\n",
            "8/8 [==============================] - 0s 52ms/step - loss: 3.3855 - accuracy: 0.0833\n",
            "Epoch 16/200\n",
            "8/8 [==============================] - 0s 50ms/step - loss: 3.3671 - accuracy: 0.0556\n",
            "Epoch 17/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 3.3835 - accuracy: 0.1111\n",
            "Epoch 18/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 3.3886 - accuracy: 0.0556\n",
            "Epoch 19/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 3.3760 - accuracy: 0.0833\n",
            "Epoch 20/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 3.3581 - accuracy: 0.0556\n",
            "Epoch 21/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 3.3651 - accuracy: 0.0833\n",
            "Epoch 22/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 3.3678 - accuracy: 0.0833\n",
            "Epoch 23/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 3.3130 - accuracy: 0.0556\n",
            "Epoch 24/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 3.3971 - accuracy: 0.0833\n",
            "Epoch 25/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 3.4026 - accuracy: 0.0556\n",
            "Epoch 26/200\n",
            "8/8 [==============================] - 0s 50ms/step - loss: 3.3556 - accuracy: 0.0278\n",
            "Epoch 27/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 3.3554 - accuracy: 0.0833\n",
            "Epoch 28/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 3.3661 - accuracy: 0.0833\n",
            "Epoch 29/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 3.3669 - accuracy: 0.0833\n",
            "Epoch 30/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 3.3494 - accuracy: 0.0833\n",
            "Epoch 31/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 3.3513 - accuracy: 0.0833\n",
            "Epoch 32/200\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 3.3448 - accuracy: 0.0556\n",
            "Epoch 33/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 3.3485 - accuracy: 0.0833\n",
            "Epoch 34/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 3.3590 - accuracy: 0.0833\n",
            "Epoch 35/200\n",
            "8/8 [==============================] - 0s 43ms/step - loss: 3.3277 - accuracy: 0.0833\n",
            "Epoch 36/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 3.3708 - accuracy: 0.0833\n",
            "Epoch 37/200\n",
            "8/8 [==============================] - 0s 50ms/step - loss: 3.3449 - accuracy: 0.0833\n",
            "Epoch 38/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 3.2904 - accuracy: 0.0833\n",
            "Epoch 39/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 3.2854 - accuracy: 0.0833\n",
            "Epoch 40/200\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 3.3032 - accuracy: 0.1111\n",
            "Epoch 41/200\n",
            "8/8 [==============================] - 0s 50ms/step - loss: 3.2942 - accuracy: 0.0833\n",
            "Epoch 42/200\n",
            "8/8 [==============================] - 0s 50ms/step - loss: 3.3080 - accuracy: 0.0833\n",
            "Epoch 43/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 3.2929 - accuracy: 0.1111\n",
            "Epoch 44/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 3.2406 - accuracy: 0.1111\n",
            "Epoch 45/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 3.2634 - accuracy: 0.1111\n",
            "Epoch 46/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 3.3026 - accuracy: 0.0833\n",
            "Epoch 47/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 3.2577 - accuracy: 0.1389\n",
            "Epoch 48/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 3.1795 - accuracy: 0.1389\n",
            "Epoch 49/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 3.1080 - accuracy: 0.1667\n",
            "Epoch 50/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 3.1584 - accuracy: 0.1111\n",
            "Epoch 51/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 3.0821 - accuracy: 0.0833\n",
            "Epoch 52/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 3.0897 - accuracy: 0.0833\n",
            "Epoch 53/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 3.0246 - accuracy: 0.0833\n",
            "Epoch 54/200\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 3.0903 - accuracy: 0.1111\n",
            "Epoch 55/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 3.1670 - accuracy: 0.0833\n",
            "Epoch 56/200\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 2.9960 - accuracy: 0.1389\n",
            "Epoch 57/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 3.0610 - accuracy: 0.0833\n",
            "Epoch 58/200\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 3.0366 - accuracy: 0.0833\n",
            "Epoch 59/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 2.9481 - accuracy: 0.1111\n",
            "Epoch 60/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 2.9365 - accuracy: 0.1111\n",
            "Epoch 61/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 2.8973 - accuracy: 0.1667\n",
            "Epoch 62/200\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 2.8557 - accuracy: 0.1944\n",
            "Epoch 63/200\n",
            "8/8 [==============================] - 0s 51ms/step - loss: 3.1408 - accuracy: 0.0556\n",
            "Epoch 64/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 2.9461 - accuracy: 0.0833\n",
            "Epoch 65/200\n",
            "8/8 [==============================] - 0s 50ms/step - loss: 3.0017 - accuracy: 0.1667\n",
            "Epoch 66/200\n",
            "8/8 [==============================] - 0s 49ms/step - loss: 3.0257 - accuracy: 0.1111\n",
            "Epoch 67/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 2.9534 - accuracy: 0.1944\n",
            "Epoch 68/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 2.8760 - accuracy: 0.1667\n",
            "Epoch 69/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 3.0011 - accuracy: 0.1111\n",
            "Epoch 70/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 2.8772 - accuracy: 0.1389\n",
            "Epoch 71/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 2.8948 - accuracy: 0.1389\n",
            "Epoch 72/200\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 2.7931 - accuracy: 0.1389\n",
            "Epoch 73/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 2.9195 - accuracy: 0.1111\n",
            "Epoch 74/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 2.7877 - accuracy: 0.1111\n",
            "Epoch 75/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 2.8629 - accuracy: 0.1111\n",
            "Epoch 76/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 2.9988 - accuracy: 0.0833\n",
            "Epoch 77/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 2.7724 - accuracy: 0.1944\n",
            "Epoch 78/200\n",
            "8/8 [==============================] - 0s 50ms/step - loss: 2.7343 - accuracy: 0.2222\n",
            "Epoch 79/200\n",
            "8/8 [==============================] - 0s 49ms/step - loss: 2.8327 - accuracy: 0.1389\n",
            "Epoch 80/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 2.7371 - accuracy: 0.1667\n",
            "Epoch 81/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 2.8262 - accuracy: 0.1667\n",
            "Epoch 82/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 2.7779 - accuracy: 0.1944\n",
            "Epoch 83/200\n",
            "8/8 [==============================] - 0s 49ms/step - loss: 2.7249 - accuracy: 0.1389\n",
            "Epoch 84/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 2.7536 - accuracy: 0.1944\n",
            "Epoch 85/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 2.6507 - accuracy: 0.1667\n",
            "Epoch 86/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 2.8329 - accuracy: 0.1667\n",
            "Epoch 87/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 2.9357 - accuracy: 0.1667\n",
            "Epoch 88/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 2.7308 - accuracy: 0.1389\n",
            "Epoch 89/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 2.8357 - accuracy: 0.1389\n",
            "Epoch 90/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 2.7034 - accuracy: 0.1389\n",
            "Epoch 91/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 2.8132 - accuracy: 0.1389\n",
            "Epoch 92/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 2.5430 - accuracy: 0.2222\n",
            "Epoch 93/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 2.6173 - accuracy: 0.1944\n",
            "Epoch 94/200\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 2.7867 - accuracy: 0.1944\n",
            "Epoch 95/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 2.6380 - accuracy: 0.2500\n",
            "Epoch 96/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 2.5809 - accuracy: 0.1667\n",
            "Epoch 97/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 2.7027 - accuracy: 0.1111\n",
            "Epoch 98/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 2.6765 - accuracy: 0.1667\n",
            "Epoch 99/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 2.6106 - accuracy: 0.1667\n",
            "Epoch 100/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 2.6953 - accuracy: 0.1111\n",
            "Epoch 101/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 2.5587 - accuracy: 0.2222\n",
            "Epoch 102/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 2.6061 - accuracy: 0.2500\n",
            "Epoch 103/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 2.6270 - accuracy: 0.1944\n",
            "Epoch 104/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 2.4603 - accuracy: 0.2222\n",
            "Epoch 105/200\n",
            "8/8 [==============================] - 0s 51ms/step - loss: 2.6442 - accuracy: 0.1944\n",
            "Epoch 106/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 2.5424 - accuracy: 0.3056\n",
            "Epoch 107/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 2.4796 - accuracy: 0.1944\n",
            "Epoch 108/200\n",
            "8/8 [==============================] - 0s 50ms/step - loss: 2.4633 - accuracy: 0.1944\n",
            "Epoch 109/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 2.3406 - accuracy: 0.1667\n",
            "Epoch 110/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 2.5316 - accuracy: 0.2222\n",
            "Epoch 111/200\n",
            "8/8 [==============================] - 0s 53ms/step - loss: 2.4500 - accuracy: 0.1667\n",
            "Epoch 112/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 2.2765 - accuracy: 0.2778\n",
            "Epoch 113/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 2.4486 - accuracy: 0.1944\n",
            "Epoch 114/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 2.3521 - accuracy: 0.1944\n",
            "Epoch 115/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 2.2812 - accuracy: 0.2222\n",
            "Epoch 116/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 2.5925 - accuracy: 0.1944\n",
            "Epoch 117/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 2.3461 - accuracy: 0.3333\n",
            "Epoch 118/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 2.2716 - accuracy: 0.2778\n",
            "Epoch 119/200\n",
            "8/8 [==============================] - 0s 50ms/step - loss: 2.3475 - accuracy: 0.2778\n",
            "Epoch 120/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 2.2836 - accuracy: 0.3056\n",
            "Epoch 121/200\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 2.2663 - accuracy: 0.2500\n",
            "Epoch 122/200\n",
            "8/8 [==============================] - 0s 49ms/step - loss: 2.2018 - accuracy: 0.3056\n",
            "Epoch 123/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 2.0681 - accuracy: 0.3056\n",
            "Epoch 124/200\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 1.9193 - accuracy: 0.3889\n",
            "Epoch 125/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 2.1369 - accuracy: 0.3889\n",
            "Epoch 126/200\n",
            "8/8 [==============================] - 0s 52ms/step - loss: 2.1010 - accuracy: 0.3333\n",
            "Epoch 127/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 2.1571 - accuracy: 0.2500\n",
            "Epoch 128/200\n",
            "8/8 [==============================] - 0s 49ms/step - loss: 2.2733 - accuracy: 0.2222\n",
            "Epoch 129/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 2.2402 - accuracy: 0.2778\n",
            "Epoch 130/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 2.2371 - accuracy: 0.2778\n",
            "Epoch 131/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 2.0203 - accuracy: 0.4167\n",
            "Epoch 132/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 2.4142 - accuracy: 0.2500\n",
            "Epoch 133/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 1.9995 - accuracy: 0.3889\n",
            "Epoch 134/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 2.0517 - accuracy: 0.3611\n",
            "Epoch 135/200\n",
            "8/8 [==============================] - 0s 53ms/step - loss: 1.8943 - accuracy: 0.4167\n",
            "Epoch 136/200\n",
            "8/8 [==============================] - 0s 50ms/step - loss: 2.2134 - accuracy: 0.2500\n",
            "Epoch 137/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 1.8177 - accuracy: 0.4444\n",
            "Epoch 138/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 1.8074 - accuracy: 0.3611\n",
            "Epoch 139/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 1.9355 - accuracy: 0.3333\n",
            "Epoch 140/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 1.8416 - accuracy: 0.3611\n",
            "Epoch 141/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 1.7220 - accuracy: 0.5000\n",
            "Epoch 142/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 1.9824 - accuracy: 0.3611\n",
            "Epoch 143/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 1.7815 - accuracy: 0.4444\n",
            "Epoch 144/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 1.4842 - accuracy: 0.5000\n",
            "Epoch 145/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 2.1054 - accuracy: 0.3056\n",
            "Epoch 146/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 1.6849 - accuracy: 0.4167\n",
            "Epoch 147/200\n",
            "8/8 [==============================] - 0s 49ms/step - loss: 1.9355 - accuracy: 0.3889\n",
            "Epoch 148/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 1.6289 - accuracy: 0.4167\n",
            "Epoch 149/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 1.8412 - accuracy: 0.3889\n",
            "Epoch 150/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 1.8244 - accuracy: 0.3889\n",
            "Epoch 151/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 1.7203 - accuracy: 0.4167\n",
            "Epoch 152/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 1.4061 - accuracy: 0.5833\n",
            "Epoch 153/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 1.6828 - accuracy: 0.3889\n",
            "Epoch 154/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 1.5839 - accuracy: 0.4722\n",
            "Epoch 155/200\n",
            "8/8 [==============================] - 0s 51ms/step - loss: 1.6311 - accuracy: 0.3889\n",
            "Epoch 156/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 1.5777 - accuracy: 0.4167\n",
            "Epoch 157/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 1.4552 - accuracy: 0.3889\n",
            "Epoch 158/200\n",
            "8/8 [==============================] - 0s 49ms/step - loss: 1.7221 - accuracy: 0.3889\n",
            "Epoch 159/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 1.5576 - accuracy: 0.4722\n",
            "Epoch 160/200\n",
            "8/8 [==============================] - 0s 57ms/step - loss: 1.4804 - accuracy: 0.5000\n",
            "Epoch 161/200\n",
            "8/8 [==============================] - 0s 49ms/step - loss: 1.1548 - accuracy: 0.6111\n",
            "Epoch 162/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 1.4506 - accuracy: 0.5000\n",
            "Epoch 163/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 1.5409 - accuracy: 0.3333\n",
            "Epoch 164/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 1.3713 - accuracy: 0.5000\n",
            "Epoch 165/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 1.5285 - accuracy: 0.4444\n",
            "Epoch 166/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 1.6806 - accuracy: 0.3889\n",
            "Epoch 167/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 1.2399 - accuracy: 0.5278\n",
            "Epoch 168/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 1.5435 - accuracy: 0.5556\n",
            "Epoch 169/200\n",
            "8/8 [==============================] - 0s 49ms/step - loss: 1.5286 - accuracy: 0.5000\n",
            "Epoch 170/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 1.2119 - accuracy: 0.5556\n",
            "Epoch 171/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 1.4544 - accuracy: 0.4722\n",
            "Epoch 172/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 1.4570 - accuracy: 0.5000\n",
            "Epoch 173/200\n",
            "8/8 [==============================] - 0s 50ms/step - loss: 1.3044 - accuracy: 0.5556\n",
            "Epoch 174/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 1.5643 - accuracy: 0.4167\n",
            "Epoch 175/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 1.4266 - accuracy: 0.3611\n",
            "Epoch 176/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 1.1989 - accuracy: 0.6111\n",
            "Epoch 177/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 1.1190 - accuracy: 0.5556\n",
            "Epoch 178/200\n",
            "8/8 [==============================] - 0s 50ms/step - loss: 1.4009 - accuracy: 0.5000\n",
            "Epoch 179/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 1.1085 - accuracy: 0.6111\n",
            "Epoch 180/200\n",
            "8/8 [==============================] - 0s 49ms/step - loss: 1.2504 - accuracy: 0.5833\n",
            "Epoch 181/200\n",
            "8/8 [==============================] - 0s 49ms/step - loss: 1.2961 - accuracy: 0.5556\n",
            "Epoch 182/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 1.3037 - accuracy: 0.5556\n",
            "Epoch 183/200\n",
            "8/8 [==============================] - 0s 55ms/step - loss: 1.2951 - accuracy: 0.5278\n",
            "Epoch 184/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 1.3031 - accuracy: 0.5278\n",
            "Epoch 185/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 1.5293 - accuracy: 0.4722\n",
            "Epoch 186/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 1.4538 - accuracy: 0.5556\n",
            "Epoch 187/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 1.5106 - accuracy: 0.4722\n",
            "Epoch 188/200\n",
            "8/8 [==============================] - 0s 49ms/step - loss: 1.4047 - accuracy: 0.5000\n",
            "Epoch 189/200\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 1.5026 - accuracy: 0.5000\n",
            "Epoch 190/200\n",
            "8/8 [==============================] - 0s 53ms/step - loss: 1.1578 - accuracy: 0.6389\n",
            "Epoch 191/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 1.2792 - accuracy: 0.4167\n",
            "Epoch 192/200\n",
            "8/8 [==============================] - 0s 50ms/step - loss: 1.2168 - accuracy: 0.5556\n",
            "Epoch 193/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 1.2089 - accuracy: 0.5833\n",
            "Epoch 194/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 1.2019 - accuracy: 0.5833\n",
            "Epoch 195/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 1.2572 - accuracy: 0.5833\n",
            "Epoch 196/200\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 1.1028 - accuracy: 0.6667\n",
            "Epoch 197/200\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 1.1096 - accuracy: 0.6111\n",
            "Epoch 198/200\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 1.0491 - accuracy: 0.6944\n",
            "Epoch 199/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 1.1972 - accuracy: 0.6111\n",
            "Epoch 200/200\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 1.1060 - accuracy: 0.5833\n"
          ]
        }
      ]
    }
  ]
}